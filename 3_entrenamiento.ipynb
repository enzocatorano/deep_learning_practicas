{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx/ljB4EhUJFOyuiRDD0cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enzocatorano/deep_learning_practicas/blob/master/3_entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Ejercicio 1**: Inicialización de pesos"
      ],
      "metadata": {
        "id": "w0Kptpjf1xv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio, vamos a probar distintas inicializaciones de pesos para la red `IrisMLP`. Para esto, modificar el constructor de la red de manera que se pueda pasar el método de inicialización como argumento. Pueden definir un método privado `_init_weights` por ejemplo y llamarlo en el constructor (el prefijo `_` es la convención en Python para métodos privados).\n",
        "\n",
        "a) Probemos inicializar la red `IrisMLP` con todos ceros. Observar qué ocurre con la evolución de los pesos dentro de una misma capa (para lo cual puede ser útil Tensorboard), y con el desempeño final de la red.\n",
        "\n",
        "b) Implementar la inicialización Xavier, He y con una distribución normal.\n",
        "\n",
        "Opcionalmente, pueden repetir estos incisos para `NetCNN`.\n"
      ],
      "metadata": {
        "id": "yCyTfhBm1zuM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt0OKllHRbM1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "from torch.optim import Optimizer\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class PerceptronMulticapa (nn.Module):\n",
        "\n",
        "  #############################################################################################################\n",
        "  def __init__ (self, n_entrada, n_oculta, n_salida, f_act = 'relu',\n",
        "                semilla = None, metodo_init_pesos = None, batch_norm = False,\n",
        "                dropout_val = 0.0):\n",
        "    super(PerceptronMulticapa, self).__init__()\n",
        "    if semilla is not None:\n",
        "      torch.manual_seed(semilla)\n",
        "      self.semilla = semilla\n",
        "    self.n_entrada = n_entrada\n",
        "    self.n_oculta = n_oculta\n",
        "    self.n_salida = n_salida\n",
        "    if f_act == 'relu':\n",
        "      self.f_act = nn.ReLU()\n",
        "    elif f_act == 'sigmoid':\n",
        "      self.f_act = nn.Sigmoid()\n",
        "    self.fc1 = torch.nn.Linear(n_entrada, n_oculta, dtype = torch.float32)\n",
        "    self.fc2 = torch.nn.Linear(n_oculta, n_salida, dtype = torch.float32)\n",
        "    if metodo_init_pesos is not None:\n",
        "      self._init_pesos(metodo_init_pesos)\n",
        "    # necesito agregar implementacion para el escenario en que se use\n",
        "    # normalizacion por lotes\n",
        "    def mantener (valor):\n",
        "      return valor\n",
        "    if batch_norm:\n",
        "      self.bn1 = nn.BatchNorm1d(n_oculta)\n",
        "      self.bn2 = nn.BatchNorm1d(n_salida)\n",
        "    else: # si no hay que hacer BN, las guarda como funciones que no hacen nada\n",
        "      self.bn1 = mantener\n",
        "      self.bn2 = mantener\n",
        "    # agrego la posibilidad de implementar dropout\n",
        "    if dropout_val > 0:\n",
        "      self.dropout1 = nn.Dropout(dropout_val)\n",
        "      self.dropout2 = nn.Dropout(dropout_val)\n",
        "    else:\n",
        "      self.dropout1 = mantener\n",
        "      self.dropout2 = mantener\n",
        "\n",
        "  ###################\n",
        "  def __str__ (self):\n",
        "    return f\"MLP con {self.n_entrada} entradas, {self.n_oculta} ocultas y {self.n_salida} salidas. Funcion de activación {self.f_act}.\"\n",
        "\n",
        "  ##############################\n",
        "  def forward (self, x_entrada):\n",
        "    if len(x_entrada.shape) == 1:\n",
        "      x_entrada = x_entrada.unsqueeze(0)\n",
        "    if x_entrada.shape[1] != self.n_entrada:\n",
        "      raise ValueError(f\"Dimensión incorrecta: se esperaban {self.n_entrada} entradas por dato.\")\n",
        "    y1 = self.fc1(x_entrada)\n",
        "    z1 = self.dropout1(self.bn1(self.f_act(y1)))\n",
        "    y2 = self.fc2(z1)\n",
        "    z2 = self.dropout2(self.bn2(self.f_act(y2)))\n",
        "    return z2\n",
        "\n",
        "  ##########################################\n",
        "  def _init_pesos (self, metodo_init_pesos):\n",
        "    '''\n",
        "    Funcion de inicializacion de pesos, espera un metodo de inicializacion como argumento\n",
        "    del tipo torch.nn.init.Module. Como ser:\n",
        "    - torch.nn.init.zeros_\n",
        "    - torch.nn.init.xavier_uniform_\n",
        "    - torch.nn.init.xavier_normal_\n",
        "    - torch.nn.init.kaiming_uniform_\n",
        "    - torch.nn.init.kaiming_normal_\n",
        "    '''\n",
        "    for capa in [self.fc1, self.fc2]:\n",
        "      metodo_init_pesos(capa.weight)\n",
        "      if capa.bias is not None:\n",
        "        capa.bias.data.fill_(0) # pone ceros para los sesgos, lo que es comun\n",
        "\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "\n",
        "class NetCNN(nn.Module):\n",
        "\n",
        "  ############################################################################\n",
        "  def __init__ (self, f1 = 5, s1 = 1, p1 = 0, f2 = 5, s2 = 1, p2 = 0,\n",
        "                semilla = None, metodo_init_pesos = None, batch_norm = False,\n",
        "                dropout_val = 0.0):\n",
        "    '''\n",
        "    Hay parametros no manejables, pues, se corresponden a aquellos dados por consigna.\n",
        "    Como ser el tipo y tamaño del pooling, la cantidad de features map antes y despues\n",
        "    de cada capa convolucional, las neuronas de cada capa MLP, y las funciones de activacion.\n",
        "    '''\n",
        "    super(NetCNN, self).__init__()\n",
        "    if semilla != None:\n",
        "      torch.manual_seed(semilla)\n",
        "      self.semilla = semilla\n",
        "    self.fm0 = 1\n",
        "    self.fm1 = 6\n",
        "    self.fm2 = 16\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv1 = nn.Conv2d(self.fm0, self.fm1, f1, s1, p1)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) # este es el reductor de dimensiones de los features maps\n",
        "    self.conv2 = nn.Conv2d(self.fm1, self.fm2, f2, s2, p2)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(self.fm2 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    if metodo_init_pesos is not None:\n",
        "      self._init_pesos(metodo_init_pesos)\n",
        "    def mantener (valor):\n",
        "      return valor\n",
        "    if batch_norm:\n",
        "      self.cbn1 = nn.BatchNorm2d(self.fm1)\n",
        "      self.cbn2 = nn.BatchNorm2d(self.fm2)\n",
        "      self.lbn1 = nn.BatchNorm1d(120)\n",
        "      self.lbn2 = nn.BatchNorm1d(84)\n",
        "    else:\n",
        "      self.cbn1 = mantener\n",
        "      self.cbn2 = mantener\n",
        "      self.lbn1 = mantener\n",
        "      self.lbn2 = mantener\n",
        "    if dropout_val > 0:\n",
        "      self.dropout1 = nn.Dropout(dropout_val)\n",
        "      self.dropout2 = nn.Dropout(dropout_val)\n",
        "    else:\n",
        "      self.dropout1 = mantener\n",
        "      self.dropout2 = mantener\n",
        "\n",
        "  ##################\n",
        "  def __str__(self):\n",
        "    desc = \"NetCNN con arquitectura:\\n\"\n",
        "    desc += f\"- Conv1: entrada {self.fm0} → {self.fm1} filtros\\n\"\n",
        "    desc += f\"- Conv2: entrada {self.fm1} → {self.fm2} filtros\\n\"\n",
        "    desc += f\"- FC1: {self.fm2 * 4 * 4} → 120\\n\"\n",
        "    desc += f\"- FC2: 120 → 84\\n\"\n",
        "    desc += f\"- FC3: 84 → 10 (salidas)\\n\"\n",
        "    desc += f\"Activación: ReLU\\n\"\n",
        "    desc += f\"BatchNorm: {'sí' if hasattr(self, 'cbn1') and not callable(self.bn1) else 'no'}\"\n",
        "    return desc\n",
        "\n",
        "  ######################\n",
        "  def forward (self, x):\n",
        "    h = self.conv1(x)\n",
        "    h = self.relu(h)\n",
        "    h = self.cbn1(h)\n",
        "    h = self.pool1(h)\n",
        "    h = self.conv2(h)\n",
        "    h = self.relu(h)\n",
        "    h = self.cbn2(h)\n",
        "    h = self.pool2(h)\n",
        "    h = h.view(-1, self.fm2 * 4 * 4) # aca se aplanan todos los features maps sobre un solo vector\n",
        "    h = self.fc1(h)\n",
        "    h = self.relu(h)\n",
        "    h = self.lbn1(h)\n",
        "    h = self.dropout1(h)\n",
        "    h = self.fc2(h)\n",
        "    h = self.relu(h)\n",
        "    h = self.lbn2(h)\n",
        "    h = self.dropout2(h)\n",
        "    y = self.fc3(h)\n",
        "    return y # 10 elementos\n",
        "\n",
        "  ##########################################\n",
        "  def _init_pesos (self, metodo_init_pesos):\n",
        "    '''\n",
        "    Funcion de inicializacion de pesos, espera un metodo de inicializacion como argumento\n",
        "    del tipo torch.nn.init.Module. Como ser:\n",
        "    - torch.nn.init.zeros_\n",
        "    - torch.nn.init.xavier_uniform_\n",
        "    - torch.nn.init.xavier_normal_\n",
        "    - torch.nn.init.kaiming_uniform_\n",
        "    - torch.nn.init.kaiming_normal_\n",
        "    '''\n",
        "    for capa in [self.conv1, self.conv2, self.fc1, self.fc2, self.fc3]:\n",
        "      metodo_init_pesos(capa.weight)\n",
        "      if capa.bias is not None:\n",
        "        capa.bias.data.fill_(0) # pone ceros para los sesgos, lo que es comun\n",
        "\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "\n",
        "class Entrenador:\n",
        "    def __init__(\n",
        "        self,\n",
        "        modelo: nn.Module,\n",
        "        cargador_entrenamiento: DataLoader,\n",
        "        optimizador: Optimizer,\n",
        "        func_perdida: nn.Module,\n",
        "        cargador_validacion: DataLoader = None,\n",
        "        device: str | None = None,\n",
        "        parada_temprana: int | None = 10,\n",
        "        regularizador: tuple[int, float] = None\n",
        "    ):\n",
        "        # Configuración de dispositivo\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            print(f\"Usando dispositivo: {device.upper()}\")\n",
        "        self.device = device\n",
        "        self.modelo = modelo.to(self.device)\n",
        "        self.cargador_entrenamiento = cargador_entrenamiento\n",
        "        self.cargador_validacion = cargador_validacion\n",
        "        self.optimizador = optimizador\n",
        "        self.func_perdida = func_perdida\n",
        "        self.parada_temprana = parada_temprana\n",
        "\n",
        "        # Bandera para aplicar softmax si se usa MSELoss\n",
        "        self.usar_softmax = isinstance(self.func_perdida, nn.MSELoss)\n",
        "\n",
        "        # Validación de formato de regularizador\n",
        "        if regularizador is not None:\n",
        "            if (\n",
        "                not isinstance(regularizador, tuple)\n",
        "                or len(regularizador) != 2\n",
        "                or not isinstance(regularizador[0], int)\n",
        "                or not isinstance(regularizador[1], float)\n",
        "            ):\n",
        "                raise ValueError(\"El regularizador debe ser de formato tuple[int, float].\")\n",
        "            if regularizador[0] not in [1, 2]:\n",
        "                raise ValueError(\"El primer elemento del regularizador debe ser 1 o 2.\")\n",
        "            self.regularizador = regularizador\n",
        "        else:\n",
        "            self.regularizador = None\n",
        "\n",
        "        # Construcción de la función de pérdida total con regularización\n",
        "        if self.regularizador:\n",
        "            def func_perdida_total(salida, salida_esperada):\n",
        "                perdida = self.func_perdida(salida, salida_esperada)\n",
        "                reg = 0.0\n",
        "                for nombre, param in self.modelo.named_parameters():\n",
        "                    if \"bias\" not in nombre and param.requires_grad:\n",
        "                        reg += torch.norm(param, p=self.regularizador[0])\n",
        "                return perdida + self.regularizador[1] * reg\n",
        "            self.func_perdida_total = func_perdida_total\n",
        "        else:\n",
        "            self.func_perdida_total = self.func_perdida\n",
        "\n",
        "        # Historial de métricas\n",
        "        self.perdida_entrenamiento = []\n",
        "        self.perdida_validacion = []\n",
        "        self.precision_validacion = []\n",
        "        self.mejor_perdida_validacion = float('inf')\n",
        "        self.mejor_modelo = None\n",
        "        self.epocas_sin_mejora = 0\n",
        "\n",
        "    def ajustar(\n",
        "        self,\n",
        "        epocas: int,\n",
        "        imprimir_perdida=False,\n",
        "        plotear_perdida=True,\n",
        "        escritor=None\n",
        "    ):\n",
        "        # Chequeo inicial de formato de etiquetas\n",
        "        lote_ejemplo = next(iter(self.cargador_entrenamiento))\n",
        "        _, etiquetas_ejemplo = lote_ejemplo\n",
        "        if isinstance(self.func_perdida, nn.CrossEntropyLoss) and etiquetas_ejemplo.ndim != 1:\n",
        "            raise ValueError(\"CrossEntropyLoss espera etiquetas como índices enteros (dim = 1).\")\n",
        "        if isinstance(self.func_perdida, nn.MSELoss) and not (\n",
        "            etiquetas_ejemplo.ndim == 2 and etiquetas_ejemplo.size(1) > 1\n",
        "        ):\n",
        "            raise ValueError(\"MSELoss espera etiquetas en formato one-hot (batch_size, num_classes).\")\n",
        "\n",
        "        # Bucle de entrenamiento\n",
        "        for t in range(epocas):\n",
        "            self.modelo.train()\n",
        "            perdida_epoca = 0.0\n",
        "\n",
        "            for x_entrada, salida_esperada in self.cargador_entrenamiento:\n",
        "                x_entrada = x_entrada.to(self.device)\n",
        "                salida_esperada = salida_esperada.to(self.device)\n",
        "\n",
        "                # Forward + softmax opcional\n",
        "                salida = self.modelo(x_entrada)\n",
        "                if self.usar_softmax:\n",
        "                    salida = F.softmax(salida, dim=1)\n",
        "\n",
        "                # Cálculo de pérdida y backward\n",
        "                perdida = self.func_perdida_total(salida, salida_esperada)\n",
        "                self.optimizador.zero_grad()\n",
        "                perdida.backward()\n",
        "                self.optimizador.step()\n",
        "\n",
        "                perdida_epoca += perdida.item()\n",
        "\n",
        "            perdida_prom = perdida_epoca / len(self.cargador_entrenamiento)\n",
        "            if escritor:\n",
        "                escritor.add_scalar(\"Loss/train\", perdida_prom, t)\n",
        "                # Logueo de pesos, gradientes y BatchNorm\n",
        "                for nombre, param in self.modelo.named_parameters():\n",
        "                    escritor.add_histogram(f\"Weights/{nombre}\", param, t)\n",
        "                    if param.grad is not None:\n",
        "                        escritor.add_histogram(f\"Gradients/{nombre}\", param.grad, t)\n",
        "                    if 'bn' in nombre:\n",
        "                        escritor.add_histogram(f\"BatchNorm/{nombre}\", param, t)\n",
        "                for nombre, buffer in self.modelo.state_dict().items():\n",
        "                    if 'running_mean' in nombre or 'running_var' in nombre:\n",
        "                        escritor.add_histogram(f\"BatchNorm/{nombre}\", buffer, t)\n",
        "            else:\n",
        "                self.perdida_entrenamiento.append(perdida_prom)\n",
        "\n",
        "            if imprimir_perdida:\n",
        "                print(f\"Época {t+1}/{epocas} — Pérdida train: {perdida_prom:.4f}\")\n",
        "\n",
        "            # Validación y parada temprana\n",
        "            if self.cargador_validacion:\n",
        "                p_val, acc_val = self.validar(imprimir_perdida)\n",
        "                self.perdida_validacion.append(p_val)\n",
        "                self.precision_validacion.append(acc_val)\n",
        "\n",
        "                if escritor:\n",
        "                    escritor.add_scalar(\"Loss/val\", p_val, t)\n",
        "                    escritor.add_scalar(\"Accuracy/val\", acc_val, t)\n",
        "\n",
        "                if p_val < self.mejor_perdida_validacion:\n",
        "                    self.mejor_perdida_validacion = p_val\n",
        "                    self.guardar_checkpoint()\n",
        "                    self.epocas_sin_mejora = 0\n",
        "                else:\n",
        "                    self.epocas_sin_mejora += 1\n",
        "                if self.parada_temprana and self.epocas_sin_mejora >= self.parada_temprana:\n",
        "                    print(f\"Detenido en época {t+1}: sin mejora por {self.parada_temprana} épocas.\")\n",
        "                    break\n",
        "\n",
        "        # Gráfica de pérdidas\n",
        "        if plotear_perdida:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.style.use('dark_background')\n",
        "            plt.plot(range(1, len(self.perdida_entrenamiento)+1), self.perdida_entrenamiento, label='Train')\n",
        "            if self.cargador_validacion:\n",
        "                plt.plot(range(1, len(self.perdida_validacion)+1), self.perdida_validacion, label='Val')\n",
        "            plt.title('Pérdida vs Época')\n",
        "            plt.xlabel('Época')\n",
        "            plt.ylabel('Pérdida')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    def validar(self, imprimir_perdida=False) -> tuple[float, float]:\n",
        "        self.modelo.eval()\n",
        "        perdida_total = 0.0\n",
        "        correctas = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_entrada, salida_esperada in self.cargador_validacion:\n",
        "                x_entrada = x_entrada.to(self.device)\n",
        "                salida_esperada = salida_esperada.to(self.device)\n",
        "\n",
        "                salida = self.modelo(x_entrada)\n",
        "                if self.usar_softmax:\n",
        "                    salida = F.softmax(salida, dim=1)\n",
        "\n",
        "                perdida = self.func_perdida_total(salida, salida_esperada)\n",
        "                perdida_total += perdida.item()\n",
        "\n",
        "                # Predicciones\n",
        "                _, predicciones = torch.max(salida, dim=1)\n",
        "                # Manejo de etiquetas one-hot vs índices\n",
        "                if salida_esperada.ndim > 1 and salida_esperada.size(1) > 1:\n",
        "                    _, verdaderas = torch.max(salida_esperada, dim=1)\n",
        "                else:\n",
        "                    verdaderas = salida_esperada\n",
        "\n",
        "                correctas += (predicciones == verdaderas).sum().item()\n",
        "                total += verdaderas.size(0)\n",
        "\n",
        "        perdida_prom = perdida_total / len(self.cargador_validacion)\n",
        "        precision = correctas / total if total > 0 else 0.0\n",
        "        if imprimir_perdida:\n",
        "            print(f\"Pérdida val: {perdida_prom:.4f} — Precisión: {precision:.4f}\")\n",
        "        return perdida_prom, precision\n",
        "\n",
        "    def guardar_checkpoint(self):\n",
        "        self.mejor_modelo = self.modelo.state_dict().copy()\n",
        "\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "\n",
        "class Evaluador:\n",
        "\n",
        "  def __init__(self, modelo: torch.nn.Module, device: str = None, nombres_clases: list[str] = None):\n",
        "    self.modelo = modelo\n",
        "    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.nombres_clases = nombres_clases\n",
        "\n",
        "  def evaluar(self, cargador_datos: torch.utils.data.DataLoader, input_shape: tuple = None):\n",
        "    self.modelo.eval()\n",
        "    self.modelo.to(self.device)\n",
        "\n",
        "    todas_predicciones = []\n",
        "    todas_verdaderas = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, y in cargador_datos:\n",
        "        if input_shape:\n",
        "          x = x.view(-1, *input_shape)  # reshape explícito si se necesita\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        salida = self.modelo(x)\n",
        "        pred = torch.argmax(salida, dim=1)\n",
        "\n",
        "        todas_predicciones.extend(pred.cpu().numpy())\n",
        "        todas_verdaderas.extend(y.cpu().numpy())\n",
        "\n",
        "    print(\"Reporte de clasificación:\\n\")\n",
        "    print(classification_report(todas_verdaderas, todas_predicciones, digits=4, target_names=self.nombres_clases))\n",
        "\n",
        "    matriz_conf = confusion_matrix(todas_verdaderas, todas_predicciones)\n",
        "    etiquetas = self.nombres_clases if self.nombres_clases else [str(i) for i in range(len(matriz_conf))]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(matriz_conf, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=etiquetas, yticklabels=etiquetas)\n",
        "    plt.xlabel('Etiqueta predicha')\n",
        "    plt.ylabel('Etiqueta verdadera')\n",
        "    plt.title('Matriz de Confusión - Dataset de Prueba')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "\n",
        "class gestor ():\n",
        "\n",
        "  def __init__(self, ruta_base_drive: str = \"/content/drive/MyDrive/modelos_torch\"):\n",
        "    self.ruta_base = ruta_base_drive\n",
        "    os.makedirs(self.ruta_base, exist_ok=True)\n",
        "\n",
        "  def guardar_modelo(self, modelo: torch.nn.Module, nombre_archivo: str):\n",
        "    ruta_completa = os.path.join(self.ruta_base, nombre_archivo)\n",
        "    torch.save(modelo.state_dict(), ruta_completa)\n",
        "    print(f\"Modelo guardado en: {ruta_completa}\")\n",
        "\n",
        "  def cargar_modelo(self, modelo: torch.nn.Module, nombre_archivo: str, map_location=None):\n",
        "    ruta_completa = os.path.join(self.ruta_base, nombre_archivo)\n",
        "    if not os.path.exists(ruta_completa):\n",
        "      raise FileNotFoundError(f\"No se encontró el archivo: {ruta_completa}\")\n",
        "\n",
        "    modelo.load_state_dict(torch.load(ruta_completa, map_location=map_location))\n",
        "    modelo.eval()  # Modo evaluación por defecto\n",
        "    print(f\"Modelo cargado desde: {ruta_completa}\")\n",
        "    return modelo\n",
        "\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "##########################################################################################################################################\n",
        "\n",
        "class MiDataset (Dataset):\n",
        "\n",
        "  ##################################\n",
        "  def __init__ (self, ruta_archivo, normalizar = False):\n",
        "    self.data = pd.read_csv(ruta_archivo)\n",
        "    self.etiquetas = ['Iris-setosa','Iris-versicolor','Iris-virginica']\n",
        "\n",
        "    self.X = self.data.iloc[:, :-1].values # esto los vuelve arrays numpy\n",
        "    self.Y = self.data.iloc[:, -1].values\n",
        "\n",
        "    if normalizar:\n",
        "      self.normalizador = MinMaxScaler()\n",
        "      self.X = self.normalizador.fit_transform(self.X)\n",
        "\n",
        "            # aca hay 2 'hints', no afectan al codigo, son de ayuda al usuario\n",
        "  #############################################\n",
        "  def __getitem__ (self, indice: int) -> Tuple:\n",
        "    x = torch.tensor(self.X[indice], dtype = torch.float32)\n",
        "    y = torch.zeros(3, dtype=torch.float32)\n",
        "    indice_etiqueta = self.etiquetas.index(self.Y[indice])\n",
        "    y[indice_etiqueta] = 1.0\n",
        "    return x, y\n",
        "\n",
        "  ###################\n",
        "  def __len__ (self):\n",
        "    return self.data.shape[0]\n",
        "\n",
        "  #########################################################################\n",
        "  def particionar (self, ratio_e = 0.7, semilla = None, validacion = True):\n",
        "    if ratio_e >= 1:\n",
        "      raise ValueError(f'Ratio mal definido. Obervar valor.')\n",
        "    indices = list(range(len(self)))\n",
        "    i_entrenamiento, i_resto = train_test_split(\n",
        "        indices, train_size = ratio_e, random_state = semilla)\n",
        "    if validacion == True:\n",
        "      i_validacion, i_prueba = train_test_split(\n",
        "      i_resto, train_size = 0.5, random_state = semilla)\n",
        "      datos_entrenamiento = Subset(self, i_entrenamiento)\n",
        "      datos_validacion = Subset(self, i_validacion)\n",
        "      datos_prueba = Subset(self, i_prueba)\n",
        "      return datos_entrenamiento, datos_validacion, datos_prueba\n",
        "    else:\n",
        "      i_prueba = i_resto\n",
        "      datos_entrenamiento = Subset(self, i_entrenamiento)\n",
        "      datos_prueba = Subset(self, i_prueba)\n",
        "      return datos_entrenamiento, datos_prueba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eRoaoWP526jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workdir = '/content/drive/MyDrive/Uni/Deep_Learning/Modelos'\n",
        "os.makedirs(workdir, exist_ok = True)\n",
        "\n",
        "ruta = '/content/drive/MyDrive/Uni/Deep_Learning/Iris.csv'\n",
        "iris = MiDataset(ruta)\n",
        "entrenamiento, validacion, prueba = iris.particionar(\n",
        "    ratio_e = 0.7, validacion = True)"
      ],
      "metadata": {
        "id": "SXlVG6lccXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in entrenamiento:\n",
        "  print(x.shape, y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "i8e6rR_cOcWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para IRIS habia usado un MLP de forma n_entrada x 15 x n_salida con ReLu\n",
        "n_entrada = entrenamiento[0][0].shape[0]\n",
        "n_oculta = 15\n",
        "n_salida = entrenamiento[0][1].shape[0]\n",
        "#metodo_pesos = torch.nn.init.zeros_\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "#metodo_pesos = None\n",
        "semilla = None\n",
        "\n",
        "modelo_init_ceros = PerceptronMulticapa(n_entrada, n_oculta, n_salida, f_act = 'relu',\n",
        "                                        metodo_init_pesos = metodo_pesos, semilla = semilla)\n",
        "\n",
        "log_dir = f\"runs/iris_{int(time.time())}\"\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "lr = 0.01\n",
        "optimizador = torch.optim.SGD(modelo_init_ceros.parameters(), lr = lr)\n",
        "func_perdida = nn.MSELoss(reduction = 'mean')\n",
        "epocas = 1000\n",
        "\n",
        "entrenador = Entrenador(modelo_init_ceros, entrenamiento, optimizador, func_perdida, validacion)\n",
        "entrenador.ajustar(epocas, imprimir_perdida = False,\n",
        "                   plotear_perdida = False, escritor = writer)\n",
        "writer.close() # esto podria (quiza?) estar dentro de la funcion ajustar\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "8DWiqbFoar_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejercicio 2**: regularización $L_1$ y $L_2$ sobre $\\textbf{w}$\n",
        "\n",
        "Ahora vamos a agregar regularización sobre los pesos de la red, mediante un término extra en la función de pérdida. Notar que esto no es una propiedad del modelo en sí, por lo tanto no implica cambios en el `nn.Module`. Si usan un `Trainer`, pueden por ejemplo agregar esta opción como argumento al mismo.\n",
        "\n",
        "a) Vamos a usar regularización $L_2$. Esto se puede hacer \"manualmente\" agregando el término en el bucle de entrenamiento, o agregando un `weight_decay` al optimizador (el valor que se le pasa a este argumento es simplemente el coeficiente $\\lambda$ que multiplica al término $||\\textbf{w}||_2^2$). Examinar el comportamiento de los pesos cuando se varía $\\lambda$, desde valores chicos hasta valores grandes.\n",
        "\n",
        "b) Ahora probemos con regularización $L_1$. Éste tipo de regularización se debe implementar manualmente ya que no viene como opción en el optimizador. Nuevamente, observar qué ocurre con los pesos de la red al variar el coeficiente de este término (usando Tensorboard).\n",
        "\n",
        "Nuevamente, pueden repetir estos pasos para `NetCNN` de manera opcional."
      ],
      "metadata": {
        "id": "4Fk-FgI8R5Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo queda igual que antes, salvo porque hay que especificar la regularizacion en el Entrenador\n",
        "\n",
        "# para IRIS habia usado un MLP de forma n_entrada x 15 x n_salida con ReLu\n",
        "n_entrada = entrenamiento[0][0].shape[0]\n",
        "n_oculta = 15\n",
        "n_salida = entrenamiento[0][1].shape[0]\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "semilla = None\n",
        "\n",
        "modelo = PerceptronMulticapa(n_entrada, n_oculta, n_salida, f_act = 'relu',\n",
        "                                        metodo_init_pesos = metodo_pesos, semilla = semilla)\n",
        "\n",
        "log_dir = f\"runs/iris_{int(time.time())}\"\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "lr = 0.01\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr = lr)\n",
        "func_perdida = nn.MSELoss(reduction = 'mean')\n",
        "epocas = 1000\n",
        "\n",
        "#regularizacion = None\n",
        "regularizacion = (2, 0.01) ####### aca ####### se expresa (p, lambda)\n",
        "entrenador = Entrenador(modelo, entrenamiento, optimizador, func_perdida, validacion, regularizador = regularizacion, parada_temprana = 50)\n",
        "entrenador.ajustar(epocas, imprimir_perdida = False,\n",
        "                   plotear_perdida = False, escritor = writer)\n",
        "writer.close() # esto podria (quiza?) estar dentro de la funcion ajustar\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "IClMqInxR5Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se ve en los histogramas de los pesos como van acercandose a cero conforme se dan las epocas.\n",
        "\n",
        "En L1, se nota (quiza autoconfirmado) que algunos varios se vuelven cero, por la no disminucion del gradiente.\n",
        "\n",
        "Algunas veces, el valor de los gradientes aumentaba con las epocas, cosa que no deberia si se acerca a los minimos.\n",
        "\n",
        "Parece bastante facil caer en situaciones que disparan el early stopping en pocas epocas, algo aleatorio tambien."
      ],
      "metadata": {
        "id": "WA1OUr76tPHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejercicio 3**: normalización por lotes (_batch normalization_) y _dropout_.\n",
        "\n",
        "_Nota para profes: el objetivo de las partes a y b de este ejercicio es que vean la diferencia entre aplicar BN a un MLP y a una CNN._ (`nn.BatchNorm1d(n_features)\n",
        "` vs. `nn.BatchNorm2d(num_channels)`)\n",
        "\n",
        "a) La normalización por lotes se realiza calculando estimadores de la media y la desviación estándar de las activaciones en cada lote. Sin embargo, cuando _usamos_ la red en modo evaluación no tenemos lotes necesariamente (podemos pasar de a un solo `data point` por ejemplo, en cuyo caso ni siquiera podemos estimar una desviación estándar). ¿Cómo se obtienen entonces estos valores?\n",
        "\n",
        "b) Modificar `IrisMLP` para que incluya la posibilidad de usar normalización por lotes. Deben agregar un parámetro al constructor, `use_batch_norm`, que indica si se usa o no. (_ayuda: necesitarán `torch.nn.BatchNorm1d`_)\n",
        "\n",
        "c) Modificar la `NetCNN` para que incluya normalización por lotes luego de cada capa convolucional. (_ayuda: van a necesitar `torch.nn.BatchNorm2d` ¿en qué difiere del anterior?_)\n",
        "\n",
        "d) Agregar el uso de _dropout_, con una probabilidad que se pase como argumento al constructor (si ponemos cero, se debe recuperar el comportamiento original, sin _dropout_).\n",
        "\n",
        "e) Entrenar ambas redes y comparar el desempeño. Recordar el uso de `model.train()` y `model.eval()`. ¿Qué efecto tiene cada uno? ¿Qué diferencia o relación existe entre usar el último y el gestor de contexto `with torch.no_grad()`?"
      ],
      "metadata": {
        "id": "-flR7F-W0yg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a)\n",
        "# Para casos en los que se evalua con batches de tamaño 1 (y de igual forma si es con batches mas grandes),\n",
        "# se usan los promedios y varianzas acumulados durante la etapa de entrenamiento\n",
        "\n",
        "# b)\n",
        "# pruebo usar la modificacion de batch normalization en el MLP\n",
        "\n",
        "ruta = '/content/drive/MyDrive/Uni/Deep_Learning/Iris.csv'\n",
        "iris = MiDataset(ruta, True)\n",
        "entrenamiento, validacion, prueba = iris.particionar(\n",
        "    ratio_e = 0.7, validacion = True)\n",
        "\n",
        "# aca necesito usar batches de tamaño diferente de 1, asi que:\n",
        "lote_tam = 15\n",
        "cargador_entrenamiento = DataLoader(entrenamiento, batch_size = lote_tam, shuffle = False)\n",
        "cargador_validacion = DataLoader(validacion, batch_size = lote_tam, shuffle = False)\n",
        "\n",
        "n_entrada = entrenamiento[0][0].shape[0]\n",
        "n_oculta = 15\n",
        "n_salida = entrenamiento[0][1].shape[0]\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "semilla = None\n",
        "\n",
        "modelo = PerceptronMulticapa(n_entrada, n_oculta, n_salida, f_act = 'relu',\n",
        "                             metodo_init_pesos = metodo_pesos, semilla = semilla,\n",
        "                             batch_norm = True)\n",
        "\n",
        "log_dir = f\"runs/iris_{int(time.time())}\"\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "lr = 0.01\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr = lr)\n",
        "func_perdida = nn.MSELoss(reduction = 'mean')\n",
        "epocas = 100\n",
        "\n",
        "regularizacion = (2, 0.1) ####### aca ####### se expresa (p, lambda)\n",
        "entrenador = Entrenador(modelo, cargador_entrenamiento, optimizador, func_perdida, cargador_validacion,\n",
        "                        regularizador = regularizacion, parada_temprana = 50)\n",
        "entrenador.ajustar(epocas, imprimir_perdida = False,\n",
        "                   plotear_perdida = False, escritor = writer)\n",
        "writer.close() # esto podria (quiza?) estar dentro de la funcion ajustar\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "bFVWF5uo05e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c\n",
        "# ahora voy con la CNN\n",
        "\n",
        "# cargo primero MNIST\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 10]\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "# for i in range(1,17):\n",
        "#     plt.subplot(4,4,i)\n",
        "#     plt.imshow(mnist_train.train_data[i,:,:], cmap=plt.get_cmap('gray_r'))"
      ],
      "metadata": {
        "id": "pLxlaTesNvmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = 5\n",
        "s1 = 1\n",
        "p1 = 0\n",
        "f2 = 5 # lo cambio como prueba de que pasa si reduzco el modelo\n",
        "s2 = 1\n",
        "p2 = 0\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "semilla = 17\n",
        "dropout_val = 0.1\n",
        "\n",
        "modelo = NetCNN(f1, s1, p1, f2, s2, p2, semilla = semilla,\n",
        "                metodo_init_pesos = metodo_pesos, batch_norm = True,\n",
        "                dropout_val = dropout_val)\n",
        "              # recordar que en este modelo se usa relu predefinidamente\n",
        "\n",
        "log_dir = f\"runs/mnist_{int(time.time())}\"\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "lr = 0.01\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr = lr)\n",
        "func_perdida = nn.CrossEntropyLoss()\n",
        "epocas = 5\n",
        "regularizacion = (2, 0.1) ####### aca ####### se expresa (p, lambda)\n",
        "\n",
        "lote_tam = 15\n",
        "cargador_entrenamiento = DataLoader(mnist_train, batch_size = lote_tam, shuffle = True)\n",
        "# quiero partir una parte de mnist_test para crear un cargador de validacion\n",
        "validacion = torch.utils.data.Subset(mnist_test, range(len(mnist_test)//2))\n",
        "cargador_validacion = DataLoader(validacion, batch_size = lote_tam, shuffle = False)\n",
        "\n",
        "entrenador = Entrenador(modelo, cargador_entrenamiento, optimizador, func_perdida, cargador_validacion,\n",
        "                        regularizador = regularizacion, parada_temprana = 50)\n",
        "entrenador.ajustar(epocas, imprimir_perdida = False,\n",
        "                   plotear_perdida = False, escritor = writer)\n",
        "writer.close() # esto podria (quiza?) estar dentro de la funcion ajustar\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "AYv-Ez2ElhGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La normalizacion en el caso de la red convolucional, se da entre todos los pixeles de cada uno de los features maps de un mismo canal para cada dato en un lote."
      ],
      "metadata": {
        "id": "vKLGDBjw3o_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = '/content/drive/MyDrive/Uni/Deep_Learning/Iris.csv'\n",
        "iris = MiDataset(ruta, True)\n",
        "entrenamiento, validacion, prueba = iris.particionar(\n",
        "    ratio_e = 0.7, validacion = True)\n",
        "\n",
        "# aca necesito usar batches de tamaño diferente de 1, asi que:\n",
        "lote_tam = 15\n",
        "cargador_entrenamiento = DataLoader(entrenamiento, batch_size = lote_tam, shuffle = False)\n",
        "cargador_validacion = DataLoader(validacion, batch_size = lote_tam, shuffle = False)\n",
        "\n",
        "n_entrada = entrenamiento[0][0].shape[0]\n",
        "n_oculta = 15\n",
        "n_salida = entrenamiento[0][1].shape[0]\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "dropout_val = 0.1\n",
        "semilla = 17\n",
        "\n",
        "modelo = PerceptronMulticapa(n_entrada, n_oculta, n_salida, f_act = 'relu',\n",
        "                             metodo_init_pesos = metodo_pesos, semilla = semilla,\n",
        "                             batch_norm = True, dropout_val = dropout_val)\n",
        "\n",
        "log_dir = f\"runs/iris_{int(time.time())}\"\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "lr = 0.01\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr = lr)\n",
        "func_perdida = nn.MSELoss(reduction = 'mean')\n",
        "epocas = 100\n",
        "\n",
        "regularizacion = (2, 0.1) ####### aca ####### se expresa (p, lambda)\n",
        "entrenador = Entrenador(modelo, cargador_entrenamiento, optimizador, func_perdida, cargador_validacion,\n",
        "                        regularizador = regularizacion, parada_temprana = 50)\n",
        "entrenador.ajustar(epocas, imprimir_perdida = False,\n",
        "                   plotear_perdida = False, escritor = writer)\n",
        "writer.close() # esto podria (quiza?) estar dentro de la funcion ajustar\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "AIbcWoxI7Gap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|### **Ejercicio 4**: Optimizadores\n",
        "\n",
        "En este ejercicio vamos a probar los distintos optimizadores vistos en la teoría: SGD, SGD con momento, Adagrad, Adam y AdamW:\n",
        "\n",
        "```\n",
        "optim.SGD(model.parameters(), lr=0.01)\n",
        "optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optim.Adagrad(model.parameters(), lr=0.01)\n",
        "optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
        "optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "```\n",
        "\n",
        "**Nota**: `Adam(..., weight_decay=0.01)` y `AdamW(..., weight_decay=0.01)` no hacen exactamente lo mismo, pueden encontrar una explicación en [este post](https://stackoverflow.com/questions/64621585/pytorch-optimizer-adamw-and-adam-with-weight-decay) por ejemplo.\n",
        "\n",
        "a) En el problema de MNIST con `NetCNN`, lanzar $N$ entrenamientos con cada uno de estos optimizadores, promediar las curvas de la función de pérdida en función de las épocas y comparar la velocidad de convergencia y el desempeño final (pueden también graficar la _accuracy_ que es más fácil de interpretar).\n",
        "\n",
        "b) Opcionalmente, repetir lo mismo con `IrisMLP`."
      ],
      "metadata": {
        "id": "lUxCKgv606OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Hiperparámetros fijos\n",
        "f1, s1, p1 = 5, 1, 0\n",
        "f2, s2, p2 = 5, 1, 0\n",
        "metodo_pesos = torch.nn.init.xavier_normal_\n",
        "semilla = 111\n",
        "dropout_val = 0.1\n",
        "batch_norm = True\n",
        "lr = 0.01\n",
        "epocas = 40\n",
        "regularizacion = None\n",
        "lote_tam = 15\n",
        "func_perdida = nn.CrossEntropyLoss()\n",
        "\n",
        "# DataLoaders\n",
        "cargador_entrenamiento = DataLoader(mnist_train, batch_size=lote_tam, shuffle=True)\n",
        "valid_split = Subset(mnist_test, list(range(len(mnist_test)//2)))\n",
        "cargador_validacion = DataLoader(valid_split, batch_size=lote_tam, shuffle=False)\n",
        "\n",
        "# Lista de optimizadores a probar\n",
        "optimizadores = [\n",
        "    lambda params: torch.optim.SGD(params, lr=0.01),\n",
        "    lambda params: torch.optim.SGD(params, lr=0.01, momentum=0.9),\n",
        "    lambda params: torch.optim.Adagrad(params, lr=0.01),\n",
        "    lambda params: torch.optim.Adam(params, lr=0.001, weight_decay=0),\n",
        "    lambda params: torch.optim.AdamW(params, lr=0.001, weight_decay=0.01),\n",
        "]\n",
        "nombres = [\n",
        "    'SGD_lr0.01',\n",
        "    'SGD_mom0.9',\n",
        "    'Adagrad_lr0.01',\n",
        "    'Adam_lr0.001',\n",
        "    'AdamW_wd0.01',\n",
        "]\n",
        "\n",
        "for nombre, fabrica_opt in zip(nombres, optimizadores):\n",
        "    print(f\"\\n=== Entrenando con: {nombre} ===\")\n",
        "    # Instancio modelo y optimizador\n",
        "    modelo = NetCNN(\n",
        "        f1, s1, p1, f2, s2, p2,\n",
        "        semilla=semilla,\n",
        "        metodo_init_pesos=metodo_pesos,\n",
        "        batch_norm=batch_norm,\n",
        "        dropout_val=dropout_val\n",
        "    )\n",
        "    optimizador = fabrica_opt(modelo.parameters())\n",
        "\n",
        "    # TensorBoard\n",
        "    log_dir = f\"runs/mnist_{nombre}_{int(time.time())}\"\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    # Entrenador\n",
        "    entrenador = Entrenador(\n",
        "        modelo=modelo,\n",
        "        cargador_entrenamiento=cargador_entrenamiento,\n",
        "        optimizador=optimizador,\n",
        "        func_perdida=func_perdida,\n",
        "        cargador_validacion=cargador_validacion,\n",
        "        parada_temprana = 10,\n",
        "        regularizador=regularizacion\n",
        "    )\n",
        "    entrenador.ajustar(\n",
        "        epocas=epocas,\n",
        "        imprimir_perdida=False,\n",
        "        plotear_perdida=False,\n",
        "        escritor=writer\n",
        "    )\n",
        "    writer.close()\n",
        "\n",
        "# Una vez terminado, en un notebook:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "IiT0_Aco085X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}